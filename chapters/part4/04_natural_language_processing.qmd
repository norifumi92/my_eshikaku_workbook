# ４-４. 自然言語処理 (Natural Language Processing)

**問１**

以下のSeq2Seqの説明について、空欄（あ）～（う）に埋まる言葉として正しいものを選べ。  

Seq2Seqモデルは、入力系列を処理する（あ）と、出力系列を生成する（い）から構成される。  
（あ）は入力文を順に処理し、系列の最後の隠れ状態を（う）として生成する。このベクトルは入力文全体の意味を表し、Decoder に渡されて出力系列を生成するための初期状態として利用される。  

A. （あ）Encoder、（い）Decoder、（う）コンテキストベクトル  
B. （あ）Decoder、（い）Encoder、（う）Attention重み  
C. （あ）Encoder、（い）Decoder、（う）単語埋め込み（Embedding）  
D. （あ）Embedding、（い）Attention、（う）コンテキストベクトル  

**問２**

以下のSeq2Seqの説明について、空欄（ア）～（ウ）に埋まる言葉として正しいものを選べ。  

エンコーダへの入力系列とデコーダからの出力系列は（ア）にすることができる。
最小化する対象は、（イ） で表すことができる。
文脈の長さを固定したとき、入力系列が（ウ）とエンコードの性能を担保することができない。

A.（ア）異なる長さ（イ）+logP（yの系列|✕の系列）（ウ）短すぎる  
B.（ア） 異なる長さ（イ）-logP（yの系列|xの系列）（ウ） 長すぎる  
C.（ア）同じ重み（イ）+logP（yの系列|✕の系列）（ウ） 長すぎる  
D.（ア） 同じ重み（イ）-logP（yの系列xの系列）（ウ） 短すぎる  

**問３**

Word2Vecに関する記述として正しいものを選べ。  

A. CBOWは中心の単語から周囲の単語を予測するモデルである。  
B. Skip-gramは周囲の単語から中心の単語を予測するモデルである。  
C. ネガティブサンプリングは学習コストを削減するために導入された手法である。  
D. LSIはニューラルネットワークを用いた分散表現の学習手法である。  

**問４**

GPTに関する記述として正しいものを選べ。  

A. GPTは双方向の文脈を同時に考慮する。  
B. GPTは自己回帰型モデルであり、次の単語を順番に予測する。  
C. GPTの事前学習はMasked Language Modelingを用いる。  
D. GPTは主にFew-shot LearningよりもFine-tuningに依存して性能を発揮する。  

**問５**

Word2Vec の Skip-gram モデルにおけるネガティブサンプリングに関する説明として、以下の空欄（あ）、（い）に入る語句の組み合わせとして最も適切なものを選べ。

ネガティブサンプリングでは、正例ペアと負例ペアに対して（あ）を適用し、「正例は1、負例は0」となるように学習を行う。これは従来の全語彙に対して（い）を適用するアプローチに比べ、計算速度が大幅に上昇した。  
また、負例は（う）に基づいてサンプリングすることで、頻出語ばかりが選ばれるのを防ぎ、学習の安定性を高めている。

選択肢:  

A. （あ）シグモイド関数、（い）ソフトマックス関数、（う）語彙の出現頻度を調整した分布  
B. （あ）ソフトマックス関数、（い）シグモイド関数、（う）一様乱数  
C. （あ）ReLU、（い）ソフトマックス関数、（う）語彙の出現順序  
D. （あ）シグモイド関数、（い）ソフトマックス関数、（う）一様乱数  