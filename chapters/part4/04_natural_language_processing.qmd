# 4. 自然言語処理 (Natural Language Processing)

**問1**

以下のSeq2Seqの説明について、空欄（ア）～（ウ）に埋まる言葉として正しいものを選べ。  

エンコーダへの入力系列とデコーダからの出力系列は（ア）にすることができる。
最小化する対象は、（イ） で表すことができる。
文脈の長さを固定したとき、入力系列が（ウ）とエンコードの性能を担保することができない。

A.（ア）異なる長さ（イ）+logP（yの系列|✕の系列）（ウ）短すぎる  
B.（ア） 異なる長さ（イ）-logP（yの系列|xの系列）（ウ） 長すぎる  
C.（ア）同じ重み（イ）+logP（yの系列|✕の系列）（ウ） 長すぎる  
D.（ア） 同じ重み（イ）-logP（yの系列xの系列）（ウ） 短すぎる  

**問2**

Word2Vecに関する記述として正しいものを選べ。  

A. CBOWは中心の単語から周囲の単語を予測するモデルである。  
B. Skip-gramは周囲の単語から中心の単語を予測するモデルである。  
C. ネガティブサンプリングは学習コストを削減するために導入された手法である。  
D. LSIはニューラルネットワークを用いた分散表現の学習手法である。  

**問3**

GPTに関する記述として正しいものを選べ。  

A. GPTは双方向の文脈を同時に考慮する。  
B. GPTは自己回帰型モデルであり、次の単語を順番に予測する。  
C. GPTの事前学習はMasked Language Modelingを用いる。  
D. GPTは主にFew-shot LearningよりもFine-tuningに依存して性能を発揮する。  

**問4**

Word2VecのSkip-gramモデルは、ある単語からその周囲の単語を予測することで単語ベクトル（埋め込み）を学習する手法である。しかし語彙数が多い自然言語コーパスでは、softmax関数を使って全語彙から予測対象の単語を選ぶことは非常に高コストである。この問題を解決するために導入される技法のひとつが「ネガティブサンプリング」である。ネガティブサンプリングに関する以下の記述のうち、最も適切なものはどれか。

A. ネガティブサンプリングでは、語彙全体にsoftmaxを適用し、確率分布を正規化して損失関数を計算することで効率化を実現している。  
B. ネガティブサンプリングでは、実際に出現したコンテキスト語のみを使用し、負例を使わずに分類を行う。  
C. ネガティブサンプリングでは、中心語と正しいコンテキスト語のペアを正例として扱い、それ以外のランダムに選んだ語を負例とすることで、計算負荷を大幅に削減している。  
D. ネガティブサンプリングでは、skip-gramではなくCBOWモデルにのみ適用可能であるため、効率性の面で制限がある。