# 1. 画像認識(Image Recognition)

**問1**

ボトルネック構造を導入する目的として最も適切なものはどれか。  

A. 活性化関数の非線形性を強めるため  
B. 計算量とパラメータ数を削減するため  
C. バッチ正規化の効果を最大化するため  
D. 残差接続を無効化するため  

**問2**

入力チャネル $C_{in}=64$、出力チャネル$C_{out}=256$、特徴マップのサイズ$H=W=32$とする。  
この時、通常の3×3畳み込みブロック１層による畳み込みの計算量と、ResNet50でボトルネック構造を用いた以下の構造による畳み込みの計算量を比較すると、計算量の削減率として最も近いものはどれか。
ただし、計算量を示す指標としてはMAC数を使用することとする。  

---

1×1 conv : 64 → 64  
3×3 conv : 64 → 64  
1×1 conv : 64 → 256  

---

A. 約1/2  
B. 約1/4  
C. 約1/8  
D. 約1/16  

**問3**

ディープラーニングの画像認識モデルには、精度向上・学習安定化・パラメータ削減・計算効率化など、さまざまな設計思想がある。
例えば、ResNetは「残差接続」により勾配消失問題を解決し超深層ネットワークの学習を可能にしたが、必ずしもパラメータ削減を主目的としていない。一方で、DenseNetは「特徴マップの再利用」によりコンパクトなモデルを実現し、MobileNetは「Depthwise Separable Convolution」により軽量かつ高速な推論を可能にするなど、効率性を強く意識して設計されている。
このように、各モデルの狙いを理解することは応用研究や実装において重要である。
次の組み合わせのうち、両方のモデルが「パラメータ削減」と「計算効率化」の両方を強く意識して設計されている組み合わせはどれか。

A. VGG16 と ResNet  
B. DenseNet と ResNet  
C. GoogLeNet と MobileNet  
D. ResNet と EfficientNet  

**問4**

Vision Transformer（ViT）の特徴として正しいものを1つ選べ。  
A. 画像を1ピクセルごとに分割して系列として入力し、CNNよりもパラメータ数が少なくなる傾向がある。  
B. 画像を固定サイズのパッチに分割し、系列としてTransformerに入力するため、自己注意機構で画像全体の依存関係を学習できる。  
C. CNNの畳み込み層を多用するため、自然言語処理タスクへの応用は困難である。  
D. 入力画像サイズを大きくしてもパラメータ数が比例して増えるため、高解像度画像の学習は計算負荷が低い。



