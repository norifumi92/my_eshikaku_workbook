# 7. 深層強化学習 (Reinforcement Learning)

### 問1

あるMDP環境で以下の条件が与えられている：

---

割引率$𝛾=0.5$  
状態$s_0$で行動$𝑎_0$を選択すると、即時報酬$𝑟_0=2$を得て、状態$s_1$に遷移する。  
状態$𝑠_1$から方策𝜋に従うと、即時報酬$𝑟_1=4$を得て、遷移後は報酬が0で終わる。

---

このとき、次の値を求めなさい。

I. $𝑉^𝜋(𝑠_1)$  
II. $𝑄^𝜋(𝑠_0,𝑎_0)$

### 問2

DQNにおいて、Q学習の一部である「TD誤差」は以下のように定義される。
次の空欄（あ）に入る適切な語句を選べ。
$$
TD誤差 = [あ] − Q(s, a)
$$
A. Q(s', a') の平均値  
B. 実際に得られた報酬  
C. ${γ * Q(s, a)}$  
D. ${R(s, a) + γ * maxQ(s', a')}$

### 問3

強化学習の基本的なアルゴリズムのうちの一つとして、ベルマン方程式に従って行動価値関数$𝑄(𝑠, 𝑎)$をモデル化する価値反復に基づく方法が挙げられる。しかしこの方法では行動𝑎が連続な値を取る場合ような場合を自然に扱えないといった欠点がある。そうした問題に対応する方法として、方策自体をパラメータ𝜃を用いて$𝜋_𝜃(𝑎|𝑠)$のようにモデル化した方策勾配に基づくアルゴリズムが挙げられる。方策勾配に基づくアルゴリズムにおいては方策勾配の定理により目的関数$𝐽_𝜃 = 𝑉(𝑠_0)$の𝜃に関する勾配$∇_𝜃𝐽(𝜃)$を以下のように計算できる。
$$
∇_𝜃𝐽(𝜃) = 𝔼_{𝜋_𝜃}[(あ)]
$$

こうして計算された勾配を用いて勾配法により、方策𝜋_𝜃(𝑎|𝑠) をアップデートするのが方策勾配に
基づくアルゴリズムである。
(あ)に当てはまる式として正しいものを、以下の選択肢から選べ。

A. $∇_𝜃log𝜋_𝜃(𝑎|𝑠)𝑄^𝜋(𝑠,𝑎)$  
B. $∇_𝜃𝜋_𝜃(𝑎|𝑠)𝑄^𝜋(𝑠,𝑎)$  
C. $\frac{∇_𝜃𝜋_𝜃(𝑎|𝑠)}{𝑄^𝜋(𝑠,𝑎)}$  
D. $\frac{𝑄^𝜋(𝑠,𝑎)}{∇_𝜃𝜋_𝜃(𝑎|𝑠)}$  
