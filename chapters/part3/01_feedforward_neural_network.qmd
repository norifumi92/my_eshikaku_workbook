# ３-１. 順伝播型ネットワーク (Feedforward Neural Network)

**問１**

以下は2値分類のバイナリクロスエントロピー損失を実装するコードである。空欄（ア）に入る正しいコードを選べ。

```python
import numpy as np

def binary_crossentropy(y_true, y_pred):
    """
    y_true: 真のラベル [batch_size] (0 or 1)
    y_pred: 予測確率 [batch_size] (0~1の値)
    """
    # クリッピング
    epsilon = 1e-15
    y_pred = （ア）
    
    # バイナリクロスエントロピー計算
    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
    return loss
```

A. `np.clip(y_pred, epsilon, 1.0)`  
B. `np.clip(y_pred, 0.0, 1 - epsilon)`  
C. `np.clip(y_pred, epsilon, 1 - epsilon)`  
D. `np.maximum(y_pred, epsilon)`  

**問２**

以下は多クラス分類のソフトマックス関数を実装するコードである。数値安定性を考慮した空欄（ア）、（イ）に入る正しいコードを選べ。

```python
import numpy as np

def softmax(x):
    """
    x: ロジット [batch_size, num_classes]
    """
    # 数値安定性のため最大値を引く
    x_max = （ア）
    x_shifted = x - x_max
    
    # ソフトマックス計算
    exp_x = np.exp(x_shifted)
    sum_exp = （イ）
    
    return exp_x / sum_exp
```

A. ア: `np.max(x, axis=1, keepdims=True)`, イ: `np.sum(exp_x, axis=1, keepdims=True)`  
B. ア: `np.max(x, axis=0, keepdims=True)`, イ: `np.sum(exp_x, axis=0, keepdims=True)`  
C. ア: `np.max(x, axis=1)`, イ: `np.sum(exp_x, axis=1)`  
D. ア: `np.maximum(x, 0)`, イ: `np.sum(exp_x)`  

**問３**
以下のtanh関数のプログラムで空欄（ア）に入るものを選べ。

```python
import numpy as np

def tanh(x):
    y = （ア）
    return y
```

A. `(np.exp(x) + np.exp(-x)) / (np.exp(x) + np.exp(-x))`  
B. `(np.exp(x) + np.exp(-x)) / (np.exp(x) - np.exp(-x))`  
C. `(np.exp(x) – np.exp(-x)) / (np.exp(x) - np.exp(-x))`  
D. `(np.exp(x) – np.exp(-x)) / (np.exp(x) + np.exp(-x))`


**問４**

活性化関数について、空欄 (あ) ~ (か) に当てはまるものを選べ。
活性化関数としては、かつてシグモイド関数が広く用いられていた。シグモイド関数 $\sigma(x)$ は (あ) の式で表され、その微分は $\sigma(x)$ の記号を使うと (い) で表すことができる。しかし、近年では (う) の理由により、中間層の活性化関数としては ReLU に置き換えられることが多い。一方で、(え) という利点があるため、(お)、(か) では現在でもシグモイドが採用されている。

(あ) の選択肢:  
A. $\frac{1}{1+e^{-x}}$  
B. $\frac{e^x}{1+e^x}$  
C. $\frac{1}{1+e^{x}}$  
D. $\frac{1}{1-e^{x}}$  

(い)の選択肢:  
A. $1-\sigma(x)$  
B. $\sigma(x)(1-\sigma(x))$  
C. $x(1-x)$  
D. $e^{-x}$  

(う)の選択肢:  
A. シグモイドは計算コストが高い  
B. シグモイドは常に線形  
C. シグモイドは出力が離散的  
D. 勾配消失が起きる  

(え)の選択肢:  
A. シグモイドは負の値も出せるから確率表現に適している  
B. シグモイドは出力を $(0,1)$ の範囲に制限でき、確率として解釈できる  
C. ReLU は非線形性を持たないため、確率を出せない  
D. ReLU は常に勾配が1になるため、分類には不向き  

(お)、(か) の選択肢:  
A. ResNet の中間層活性化関数  
B. Word2Vec（Negative Sampling 損失関数）  
C. GAN の識別器（Discriminator 出力層）  
D. Transformer の Multi-Head Attention のスコア計算


