# ï¼“-ï¼’. æ·±å±¤å­¦ç¿’ã®ãŸã‚ã®æœ€é©åŒ– (Optimization)

**å•ï¼‘**

æ¬¡ã®è¨ˆç®—ã‚°ãƒ©ãƒ•ã¯ã€ãƒ­ã‚¸ãƒƒãƒˆ$z = [z_1, z_2, z_3]$ã‹ã‚‰ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—ã—ã€ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±Lã‚’æ±‚ã‚ã‚‹é †ä¼æ’­ã‚’è¡¨ã—ã¦ã„ã‚‹ã€‚

```{mermaid}
%%| fig-width: 6
%%| fig-height: 5

graph LR
    Z1[z1] ---> S[softmax]
    Z2[z2] ---> S[softmax]
    Z3[z3] ---> S[softmax]
    S ---> Yhat1[Å·1]
    S ---> Yhat2[Å·2]
    S ---> Yhat3[Å·3]
    Y1[y1=0] ---> CE[Cross-Entropy Loss L]
    Y2[y2=1] ---> CE
    Y3[y3=0] ---> CE
    Yhat1 ---> CE
    Yhat2 ---> CE
    Yhat3 ---> CE
```

I. é€†ä¼æ’­ã§$\frac{âˆ‚ğ¿}{âˆ‚ğ‘§_j}$ã‚’æ±‚ã‚ãŸã¨ãã®ä¸€èˆ¬å…¬å¼ã¨ã—ã¦æ­£ã—ã„ã‚‚ã®ã‚’é¸ã¹ã€‚  
ãŸã ã—ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã¨ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã¯ä»¥ä¸‹ã§è¡¨ã›ã‚‹ã‚‚ã®ã¨ã™ã‚‹ã€‚  

$$
\hat{y}_i = \frac{\exp(z_i)}{\sum_{k} \exp(z_k)}
$$

$$
L = - \sum_{i} y_i \log(\hat{y}_i)
$$

A. $ğ‘¦_jâˆ’\hat{y}_j$  
B. $\hat{y}_j-ğ‘¦_j$  
C. $\frac{-y_j}{\hat{y}_j}$  
D. $\hat{y}_j(1-ğ‘¦_j)$

II.3ã‚¯ãƒ©ã‚¹åˆ†é¡å•é¡Œã§ã€çœŸã®ãƒ©ãƒ™ãƒ«$y = [0,1,0]$ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®å‡ºåŠ›$\hat{y}=[0.2, 0.3, 0.5]$ã®ã¨ãã€
$\frac{âˆ‚ğ¿}{âˆ‚ğ‘§_2}$ã‚’æ±‚ã‚ã‚ˆã€‚  
A. -0.7  
B. 0.7  
C. -0.3  
D. 0.3

**å•ï¼’**

AdaGradã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ•°å¼ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¡¨ã•ã‚Œã‚‹ï¼š

$h_t = h_{t-1} + âˆ‡E(W_t) âŠ™ âˆ‡E(W_t)$  
$Î±_t = Î±_0 Ã— (1/âˆš(h_t + Îµ))$  
$W_{t+1} = W_t - Î±_t âŠ™ âˆ‡E(W_t)$  

ã“ã®æ•°å¼ã«é–¢ã™ã‚‹èª¬æ˜ã¨ã—ã¦æ­£ã—ããªã„ã‚‚ã®ã¯ã©ã‚Œã‹ã€‚

A. $h_t$ã¯å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç´¯ç©äºŒä¹—å‹¾é…ã‚’è¡¨ã—ã€ãã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã©ã‚Œã ã‘æ¿€ã—ãæ›´æ–°ã•ã‚Œã¦ããŸã‹ã‚’è¨˜éŒ²ã™ã‚‹  
B. $Î±_t$ã¯å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã”ã¨ã«é©å¿œçš„ã«èª¿æ•´ã•ã‚Œã‚‹å­¦ç¿’ç‡ã§ã€ç´¯ç©äºŒä¹—å‹¾é…ãŒå¤§ãã„ã»ã©å­¦ç¿’ç‡ã¯å°ã•ããªã‚‹  
C. $âŠ™$ã¯ã‚¢ãƒ€ãƒãƒ¼ãƒ«ç©ï¼ˆè¦ç´ ã”ã¨ã®ç©ï¼‰ã‚’è¡¨ã—ã€å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç‹¬ç«‹ã—ã¦æ‰±ã†ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹  
D. Îµã¯æ•°å€¤å®‰å®šåŒ–é …ã§ã€h_tãŒå¤§ãããªã‚Šã™ãã¦å­¦ç¿’ç‡ãŒæ¥µç«¯ã«å°ã•ããªã‚‹ã“ã¨ã‚’é˜²ã  

**å•ï¼“**

ä»¥ä¸‹ã®RMSpropã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§ç©ºæ¬„ï¼ˆã‚¢ï¼‰ã«å…¥ã‚‹ã‚‚ã®ã‚’é¸ã¹ã€‚

```python
class RMSprop:
    def __init__(self, lr=0.01, decay_rate=0.99):
        self.lr = lr
        self.decay_rate = decay_rate
        self.h = None
    
    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, param in params.items():
                self.h[key] = np.zeros_like(param)
        
        for key in params.keys():
            self.h[key] *= self.decay_rate
            self.h[key] += (ã‚¢)
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
```

ï¼ˆã‚¢ï¼‰ã®é¸æŠè‚¢  
A. `(1 - self.decay_rate) + grads[key] * grads[key]`  
B. `(1 - self.decay_rate) - grads[key] * grads[key]`  
C. `(1 - self.decay_rate) * grads[key] * grads[key]`  
D. `(1 - self.decay_rate) / (grads[key] * grads[key])`  

**å•ï¼”**

ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®å®Ÿè£…ã‚’è¨˜è¿°ã—ãŸä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å‚ç…§ã—ã€ç©ºæ¬„ã«å½“ã¦ã¯ã¾ã‚‹é¸æŠè‚¢ã‚’ãã‚Œãã‚Œ1ã¤ãšã¤é¸ã¹ã€‚ãŸã ã—ã€å®Ÿè£…å†…ã®`train_flg`ã¯ã€è¨“ç·´æ™‚ã«ã¯Trueã€äºˆæ¸¬æ™‚ã«ã¯Falseã«è¨­å®šã™ã‚‹ã‚‚ã®ã¨ã—ã€`dropout_ratio`ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’æ¶ˆå»ã™ã‚‹å‰²åˆã‚’è¡¨ã™ã‚‚ã®ã¨ã™ã‚‹ã€‚

```python
1  import numpy as np
2
3  class Dropout:
4      def __init__(self, dropout_ratio=0.5):
5          self.dropout_ratio = dropout_ratio
6          self.mask = None
7
8      def forward(self, x, train_flg=True):
9          if train_flg:
10             self.mask = ( ã‚¢ ) self.dropout_ratio
11             return ( ã‚¤ )
12         else:
13             return ( ã‚¦ )
14
15     def backward(self, dout):
16         return ( ã‚¨ )
```

ï¼ˆã‚¢ï¼‰ã®é¸æŠè‚¢  
A. `np.random.randn(*x.shape) >`  
B. `np.random.rand(*x.shape) >`  
C. `np.random.randn(*x.shape) <`  
D. `np.random.rand(*x.shape) <`  

ï¼ˆã‚¤ï¼‰ã®é¸æŠè‚¢  
A. `x * self.mask`  
B. `x * (1.0 + self.mask)`  
C. `x - self.mask`  
D. `x * (1.0 - self.mask)`  

ï¼ˆã‚¦ï¼‰ã®é¸æŠè‚¢  
A. `x * self.mask`  
B. `x * (1.0 - self.mask)`  
C. `x * self.dropout_ratio`  
D. `x * (1.0 - self.dropout_ratio)`  

ï¼ˆã‚¨ï¼‰ã®é¸æŠè‚¢  
A. `dout * self.mask`  
B. `dout * (1.0 - self.mask)`  
C. `dout * self.dropout_ratio`  
D. `dout * (1.0 - self.dropout_ratio)`  