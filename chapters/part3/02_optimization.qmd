# ３-２. 深層学習のための最適化 (Optimization)

**問１**

次の計算グラフは、ロジット$z = [z_1, z_2, z_3]$からソフトマックスを計算し、クロスエントロピー損失Lを求める順伝播を表している。

```{mermaid}
%%| fig-width: 6
%%| fig-height: 5

graph LR
    Z1[z1] ---> S[softmax]
    Z2[z2] ---> S[softmax]
    Z3[z3] ---> S[softmax]
    S ---> Yhat1[ŷ1]
    S ---> Yhat2[ŷ2]
    S ---> Yhat3[ŷ3]
    Y1[y1=0] ---> CE[Cross-Entropy Loss L]
    Y2[y2=1] ---> CE
    Y3[y3=0] ---> CE
    Yhat1 ---> CE
    Yhat2 ---> CE
    Yhat3 ---> CE
```

I. 逆伝播で$\frac{∂𝐿}{∂𝑧_j}$を求めたときの一般公式として正しいものを選べ。  
ただし、ソフトマックスとクロスエントロピー損失は以下で表せるものとする。  

$$
\hat{y}_i = \frac{\exp(z_i)}{\sum_{k} \exp(z_k)}
$$

$$
L = - \sum_{i} y_i \log(\hat{y}_i)
$$

A. $𝑦_j−\hat{y}_j$  
B. $\hat{y}_j-𝑦_j$  
C. $\frac{-y_j}{\hat{y}_j}$  
D. $\hat{y}_j(1-𝑦_j)$

II.3クラス分類問題で、真のラベル$y = [0,1,0]$、ソフトマックスの出力$\hat{y}=[0.2, 0.3, 0.5]$のとき、
$\frac{∂𝐿}{∂𝑧_2}$を求めよ。  
A. -0.7  
B. 0.7  
C. -0.3  
D. 0.3

**問２**

AdaGradアルゴリズムの数式は以下のように表される：

$h_t = h_{t-1} + ∇E(W_t) ⊙ ∇E(W_t)$  
$α_t = α_0 × (1/√(h_t + ε))$  
$W_{t+1} = W_t - α_t ⊙ ∇E(W_t)$  

この数式に関する説明として正しくないものはどれか。

A. $h_t$は各パラメータの累積二乗勾配を表し、そのパラメータがどれだけ激しく更新されてきたかを記録する  
B. $α_t$は各パラメータごとに適応的に調整される学習率で、累積二乗勾配が大きいほど学習率は小さくなる  
C. $⊙$はアダマール積（要素ごとの積）を表し、各パラメータを独立して扱うために使用される  
D. εは数値安定化項で、h_tが大きくなりすぎて学習率が極端に小さくなることを防ぐ  

**問３**

以下のRMSpropのプログラムで空欄（ア）に入るものを選べ。

```python
class RMSprop:
    def __init__(self, lr=0.01, decay_rate=0.99):
        self.lr = lr
        self.decay_rate = decay_rate
        self.h = None
    
    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, param in params.items():
                self.h[key] = np.zeros_like(param)
        
        for key in params.keys():
            self.h[key] *= self.decay_rate
            self.h[key] += (ア)
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
```

（ア）の選択肢  
A. `(1 - self.decay_rate) + grads[key] * grads[key]`  
B. `(1 - self.decay_rate) - grads[key] * grads[key]`  
C. `(1 - self.decay_rate) * grads[key] * grads[key]`  
D. `(1 - self.decay_rate) / (grads[key] * grads[key])`  

**問４**

ドロップアウトの実装を記述した以下のコードを参照し、空欄に当てはまる選択肢をそれぞれ1つずつ選べ。ただし、実装内の`train_flg`は、訓練時にはTrue、予測時にはFalseに設定するものとし、`dropout_ratio`は、ニューロンを消去する割合を表すものとする。

```python
1  import numpy as np
2
3  class Dropout:
4      def __init__(self, dropout_ratio=0.5):
5          self.dropout_ratio = dropout_ratio
6          self.mask = None
7
8      def forward(self, x, train_flg=True):
9          if train_flg:
10             self.mask = ( ア ) self.dropout_ratio
11             return ( イ )
12         else:
13             return ( ウ )
14
15     def backward(self, dout):
16         return ( エ )
```

（ア）の選択肢  
A. `np.random.randn(*x.shape) >`  
B. `np.random.rand(*x.shape) >`  
C. `np.random.randn(*x.shape) <`  
D. `np.random.rand(*x.shape) <`  

（イ）の選択肢  
A. `x * self.mask`  
B. `x * (1.0 + self.mask)`  
C. `x - self.mask`  
D. `x * (1.0 - self.mask)`  

（ウ）の選択肢  
A. `x * self.mask`  
B. `x * (1.0 - self.mask)`  
C. `x * self.dropout_ratio`  
D. `x * (1.0 - self.dropout_ratio)`  

（エ）の選択肢  
A. `dout * self.mask`  
B. `dout * (1.0 - self.mask)`  
C. `dout * self.dropout_ratio`  
D. `dout * (1.0 - self.dropout_ratio)`  