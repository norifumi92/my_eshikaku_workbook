# 解答一覧

# 第1章.数学的学習(Mach Basics)
## 1. 確率・統計 (Statistics)
問1. D

# 第2章.機械学習(Machine Learning)
## 1. 機械学習の基礎(Machine Learning Basics)
問1. I: A II: C  

# 第3章.深層学習基礎(Basic Deep-learning)
## 1. 順伝播型ネットワーク (Feedforward Neural Network)
問1. C  
問2. A  
問3. D  
問4. (あ):A  
(い):B  

::: {.callout-note title="解説"}
まずは導関数の基本公式を使う
$$
\frac{d}{dx} \frac{1}{f(x)} = - \frac{f'(x)}{f(x)^2}
$$

f(x) = $1 + e^{-x}$ と置くと、
$$
f(x) = 1 + e^{-x}, \quad f'(x) = - e^{-x}
$$

連鎖律を使って微分すると、

$$
\frac{d\sigma(x)}{dx} = \frac{d}{dx} \frac{1}{f(x)} = - \frac{f'(x)}{f(x)^2} = - \frac{-e^{-x}}{(1+e^{-x})^2} = \frac{e^{-x}}{(1+e^{-x})^2}
$$

これをσ(x)を使った形に書き換える。

$$
\sigma(x) = \frac{1}{1+e^{-x}} \implies 1 - \sigma(x) = \frac{e^{-x}}{1 + e^{-x}}
$$

よって、
$$
\frac{d}{dx}\sigma(x) = \frac{e^{-x}}{(1+e^{-x})^2} = \sigma(x)(1 - \sigma(x))
$$
:::


(う):D  
(え):B  
(お)、(か):B,C  

::: {.callout-note title="解説"}
シグモイドが使われるのは２値問題で確率を結果として得たい場合のみであることを考慮していると解きやすい。
ちなみに、(D)のTransformerのMulti-Head Attentionのスコアにはソフトマックスが使われる。
:::

## 2. 深層学習のための最適化 (Optimization)
問1. I: B II: A  
問2. D  

::: {.callout-note title="解説"}
AdaGrad の特徴は「勾配の二乗を累積」して、よく動くパラメータの学習率を抑える仕組みなので、(A)は正しいです。(D)に関して、εは数値安定化項なのは正しいですが分母に0をかけられるゼロ除算を防ぐためであり、学習率が極端に下がるのを防ぐためではありません。
:::

問3. C  
問4.(あ):B  

::: {.callout-note title="解説"}
`np.random.rand`は分布[0, 1) の一様分布に従い乱数を生成する。
一方、`np.random.randn`は平均0, 分散1の標準正規分布N(0,1)に従い乱数を生成するため、必ずしも50％の要素が0.5以上にならない。
また、Numpyの`.shape`はtupleを返すが、tuple型のままだと扱いづらいため、アスタリスク(`*`)によるアンパックが必要となる。
:::


## 3. 畳み込みニューラルネットワーク (Convolutional Neural)
問1. I: B II: B  

# 第4章.深層学習応用(Advanced Deep-learning)
## 1. 画像認識(Image Recognition)
問1. A  
問2. B  

::: {.callout-note title="解説"}
MAC数は以下で示される。暗記しても良いが、小さい特徴マップ・カーネルを考えて見ると自然に導ける。
$MAC=H_{out}​×W_{out}​×C_{in}​×C_{out}​×K^2$
:::

問3. C  
問4. B  

## 2. 物体検出(Object Detection)
問1. A  
問2. A: Region-based Convolutional Neural Network  
B: Region Proposal(領域探索)  
C: Selective Search  
問3. B

## 3. Semantic Segmentation (Semantic Segmentation)
問1. C  

## 4. 自然言語処理 (Natural Language Processing)
問1. B  
問2. C  
問3. B  
問2. C  

## 5. Recurrent Neural Network (Recurrent Neural Network)
問1. D  
問2. LSTM: 忘却ゲート(Forget Gate)、入力ゲート(Input Gate)、出力ゲート(Output Gate)、記憶セル(Memory Cell)  
     GRU: リセットゲート(Reset Gate)、更新ゲート(Update Gate)

## 6. 生成モデル (Generative Model)
問1. C  

## 7. 深層強化学習 (Reinforcement Learning)
問1. I: 4 II: 4  
問2. D  
問3. A  

## 8. 様々な学習方法 (Various Learning Methods)
問1. A  
問2. A  

## 9. 深層学習の説明性 (Explainability of Deep-learning)
問1. A  
問2. B  
問3. B

# 第5章.開発・運用環境(Infrastructure)
## 4. アクセラレーター (Accelerator)
問1. A