# 解答一覧

## 第1章.数学的学習(Math Basics)
### 1-1. 確率・統計 (Statistics)
問1. D  
問2-I. C  
問2-II. A  
問2-III. B  
問3-I. A  
問3-II. C  

## 第2章.機械学習(Machine Learning)
### 2-1. 機械学習の基礎(Machine Learning Basics)
問1-I. 
(あ):C  
(い):B  
(う):D  
問1-II. B  

::: {.callout-note title="解説"}
F1スコアは以下の式で表される。
$$
\frac{2*Recall*Precision}{Recall+Precision}
$$
:::

問2. B,C,D

## 第3章.深層学習基礎(Deep-learning Basics)
### 3-1. 順伝播型ネットワーク (Feedforward Neural Network)
問1. C  
問2. A  
問3. D  
問4. (あ):A  
(い):B  

::: {.callout-note title="解説"}
まずは導関数の基本公式を使う
$$
\frac{d}{dx} \frac{1}{f(x)} = - \frac{f'(x)}{f(x)^2}
$$

f(x) = $1 + e^{-x}$ と置くと、
$$
f(x) = 1 + e^{-x}, \quad f'(x) = - e^{-x}
$$

連鎖律を使って微分すると、

$$
\frac{d\sigma(x)}{dx} = \frac{d}{dx} \frac{1}{f(x)} = - \frac{f'(x)}{f(x)^2} = - \frac{-e^{-x}}{(1+e^{-x})^2} = \frac{e^{-x}}{(1+e^{-x})^2}
$$

これをσ(x)を使った形に書き換える。

$$
\sigma(x) = \frac{1}{1+e^{-x}} \implies 1 - \sigma(x) = \frac{e^{-x}}{1 + e^{-x}}
$$

よって、
$$
\frac{d}{dx}\sigma(x) = \frac{e^{-x}}{(1+e^{-x})^2} = \sigma(x)(1 - \sigma(x))
$$
:::


(う):D  
(え):B  
(お)、(か):B,C  

::: {.callout-note title="解説"}
シグモイドが使われるのは２値問題で確率を結果として得たい場合のみであることを考慮していると解きやすい。
ちなみに、(D)のTransformerのMulti-Head Attentionのスコアにはソフトマックスが使われる。
:::

### 3-2. 深層学習のための最適化 (Optimization)

問1-I. B  

::: {.callout-note title="解説"}
Softmax の出力$\hat{y}_i$の逆伝播で入力ロジット$z_j$で微分した結果は以下のようになる。
$$
\frac{\partial \hat{y}_i}{\partial z_j} = \hat{y}_i \, (\delta_{ij} - \hat{y}_j)
$$
これは商の公式を使って導出してもいいが、試験直前であれば暗記しておいた方が時間短縮できる。
:::

問1-II. A  
問2. D  

::: {.callout-note title="解説"}
AdaGrad の特徴は「勾配の二乗を累積」して、よく動くパラメータの学習率を抑える仕組みなので、(A)は正しいです。(D)に関して、εは数値安定化項なのは正しいですが分母に0をかけられるゼロ除算を防ぐためであり、学習率が極端に下がるのを防ぐためではありません。
:::

問3. C  
問4.(あ):B  

::: {.callout-note title="解説"}
`np.random.rand`は分布[0, 1) の一様分布に従い乱数を生成する。
一方、`np.random.randn`は平均0, 分散1の標準正規分布N(0,1)に従い乱数を生成するため、必ずしも50％の要素が0.5以上にならない。
また、Numpyの`.shape`はtupleを返すが、tuple型のままだと扱いづらいため、アスタリスク(`*`)によるアンパックが必要となる。
:::


## 3. 畳み込みニューラルネットワーク (Convolutional Neural)
問1. I: B II: B  

## 第4章.深層学習応用(Advanced Deep-learning)
### 4-1. 画像認識(Image Recognition)
問1. A  
問2. B  

::: {.callout-note title="解説"}
MAC数は以下で示される。暗記しても良いが、小さい特徴マップ・カーネルを考えて見ると自然に導ける。
$MAC=H_{out}​×W_{out}​×C_{in}​×C_{out}​×K^2$
:::

問3. C  
問4. B  

### 4-2. 物体検出(Object Detection)
問1. A  

::: {.callout-note title="解説"}
Fast R-CNNでもFaster R-CNNでもbounding box regression の誤差関数は共通で、Huber損失関数を利用している。
:::

問2. A,C,D  
問3. A  
問4. 
A: Region-based Convolutional Neural Network  
B: Region Proposal(領域探索)  
C: Selective Search  
問5. B

### 4-3. Semantic Segmentation (Semantic Segmentation)
問1. C  

### 4-4. 自然言語処理 (Natural Language Processing)
問1. B  
問2. C  
問3. B  
問4. A  

## 4-5. Recurrent Neural Network (Recurrent Neural Network)
問1. D  
問2. LSTM: 忘却ゲート(Forget Gate)、入力ゲート(Input Gate)、出力ゲート(Output Gate)、記憶セル(Memory Cell)  
     GRU: リセットゲート(Reset Gate)、更新ゲート(Update Gate)

## 6. 生成モデル (Generative Model)
問1. C  

## 7. 深層強化学習 (Reinforcement Learning)
問1. I: 4 II: 4  
問2. D  
問3. A  

## 8. 様々な学習方法 (Various Learning Methods)
問1. A  
問2. A  

## 9. 深層学習の説明性 (Explainability of Deep-learning)
問1. A  
問2. B  
問3. B,C  
問4. A  

::: {.callout-note title="解説"}
Grad-CAM では最終畳み込み層の各特徴マップが特定のクラスの識別にとってどれほど重要であるかを評価するため、勾配を測定した後、空間的に平均するグローバル平均プーリング (Global Average Pooling; GAP)を行う。
:::

問5. B  

# 第5章.開発・運用環境(Infrastructure)
## 4. アクセラレーター (Accelerator)
問1. A